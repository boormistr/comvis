{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879b6e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Labeled Faces in the Wild (LFW) dataset\n",
    "This dataset is a collection of JPEG pictures of famous people collected\n",
    "over the internet, all details are available on the official website:\n",
    "    http://vis-www.cs.umass.edu/lfw/\n",
    "\"\"\"\n",
    "# Copyright (c) 2011 Olivier Grisel <olivier.grisel@ensta.org>\n",
    "# License: BSD 3 clause\n",
    "\n",
    "from os import listdir, makedirs, remove\n",
    "from os.path import join, exists, isdir\n",
    "\n",
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "from joblib import Memory\n",
    "\n",
    "from ._base import (\n",
    "    get_data_home,\n",
    "    _fetch_remote,\n",
    "    RemoteFileMetadata,\n",
    "    load_descr,\n",
    ")\n",
    "from ..utils import Bunch\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# The original data can be found in:\n",
    "# http://vis-www.cs.umass.edu/lfw/lfw.tgz\n",
    "ARCHIVE = RemoteFileMetadata(\n",
    "    filename=\"lfw.tgz\",\n",
    "    url=\"https://ndownloader.figshare.com/files/5976018\",\n",
    "    checksum=\"055f7d9c632d7370e6fb4afc7468d40f970c34a80d4c6f50ffec63f5a8d536c0\",\n",
    ")\n",
    "\n",
    "# The original funneled data can be found in:\n",
    "# http://vis-www.cs.umass.edu/lfw/lfw-funneled.tgz\n",
    "FUNNELED_ARCHIVE = RemoteFileMetadata(\n",
    "    filename=\"lfw-funneled.tgz\",\n",
    "    url=\"https://ndownloader.figshare.com/files/5976015\",\n",
    "    checksum=\"b47c8422c8cded889dc5a13418c4bc2abbda121092b3533a83306f90d900100a\",\n",
    ")\n",
    "\n",
    "# The original target data can be found in:\n",
    "# http://vis-www.cs.umass.edu/lfw/pairsDevTrain.txt',\n",
    "# http://vis-www.cs.umass.edu/lfw/pairsDevTest.txt',\n",
    "# http://vis-www.cs.umass.edu/lfw/pairs.txt',\n",
    "TARGETS = (\n",
    "    RemoteFileMetadata(\n",
    "        filename=\"pairsDevTrain.txt\",\n",
    "        url=\"https://ndownloader.figshare.com/files/5976012\",\n",
    "        checksum=\"1d454dada7dfeca0e7eab6f65dc4e97a6312d44cf142207be28d688be92aabfa\",\n",
    "    ),\n",
    "    RemoteFileMetadata(\n",
    "        filename=\"pairsDevTest.txt\",\n",
    "        url=\"https://ndownloader.figshare.com/files/5976009\",\n",
    "        checksum=\"7cb06600ea8b2814ac26e946201cdb304296262aad67d046a16a7ec85d0ff87c\",\n",
    "    ),\n",
    "    RemoteFileMetadata(\n",
    "        filename=\"pairs.txt\",\n",
    "        url=\"https://ndownloader.figshare.com/files/5976006\",\n",
    "        checksum=\"ea42330c62c92989f9d7c03237ed5d591365e89b3e649747777b70e692dc1592\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "\n",
    "#\n",
    "# Common private utilities for data fetching from the original LFW website\n",
    "# local disk caching, and image decoding.\n",
    "#\n",
    "\n",
    "\n",
    "def _check_fetch_lfw(data_home=None, funneled=True, download_if_missing=True):\n",
    "    \"\"\"Helper function to download any missing LFW data\"\"\"\n",
    "\n",
    "    data_home = get_data_home(data_home=data_home)\n",
    "    lfw_home = join(data_home, \"lfw_home\")\n",
    "\n",
    "    if not exists(lfw_home):\n",
    "        makedirs(lfw_home)\n",
    "\n",
    "    for target in TARGETS:\n",
    "        target_filepath = join(lfw_home, target.filename)\n",
    "        if not exists(target_filepath):\n",
    "            if download_if_missing:\n",
    "                logger.info(\"Downloading LFW metadata: %s\", target.url)\n",
    "                _fetch_remote(target, dirname=lfw_home)\n",
    "            else:\n",
    "                raise IOError(\"%s is missing\" % target_filepath)\n",
    "\n",
    "    if funneled:\n",
    "        data_folder_path = join(lfw_home, \"lfw_funneled\")\n",
    "        archive = FUNNELED_ARCHIVE\n",
    "    else:\n",
    "        data_folder_path = join(lfw_home, \"lfw\")\n",
    "        archive = ARCHIVE\n",
    "\n",
    "    if not exists(data_folder_path):\n",
    "        archive_path = join(lfw_home, archive.filename)\n",
    "        if not exists(archive_path):\n",
    "            if download_if_missing:\n",
    "                logger.info(\"Downloading LFW data (~200MB): %s\", archive.url)\n",
    "                _fetch_remote(archive, dirname=lfw_home)\n",
    "            else:\n",
    "                raise IOError(\"%s is missing\" % archive_path)\n",
    "\n",
    "        import tarfile\n",
    "\n",
    "        logger.debug(\"Decompressing the data archive to %s\", data_folder_path)\n",
    "        tarfile.open(archive_path, \"r:gz\").extractall(path=lfw_home)\n",
    "        remove(archive_path)\n",
    "\n",
    "    return lfw_home, data_folder_path\n",
    "\n",
    "\n",
    "def _load_imgs(file_paths, slice_, color, resize):\n",
    "    \"\"\"Internally used to load images\"\"\"\n",
    "    try:\n",
    "        from PIL import Image\n",
    "    except ImportError:\n",
    "        raise ImportError(\n",
    "            \"The Python Imaging Library (PIL) is required to load data \"\n",
    "            \"from jpeg files. Please refer to \"\n",
    "            \"https://pillow.readthedocs.io/en/stable/installation.html \"\n",
    "            \"for installing PIL.\"\n",
    "        )\n",
    "\n",
    "    # compute the portion of the images to load to respect the slice_ parameter\n",
    "    # given by the caller\n",
    "    default_slice = (slice(0, 250), slice(0, 250))\n",
    "    if slice_ is None:\n",
    "        slice_ = default_slice\n",
    "    else:\n",
    "        slice_ = tuple(s or ds for s, ds in zip(slice_, default_slice))\n",
    "\n",
    "    h_slice, w_slice = slice_\n",
    "    h = (h_slice.stop - h_slice.start) // (h_slice.step or 1)\n",
    "    w = (w_slice.stop - w_slice.start) // (w_slice.step or 1)\n",
    "\n",
    "    if resize is not None:\n",
    "        resize = float(resize)\n",
    "        h = int(resize * h)\n",
    "        w = int(resize * w)\n",
    "\n",
    "    # allocate some contiguous memory to host the decoded image slices\n",
    "    n_faces = len(file_paths)\n",
    "    if not color:\n",
    "        faces = np.zeros((n_faces, h, w), dtype=np.float32)\n",
    "    else:\n",
    "        faces = np.zeros((n_faces, h, w, 3), dtype=np.float32)\n",
    "\n",
    "    # iterate over the collected file path to load the jpeg files as numpy\n",
    "    # arrays\n",
    "    for i, file_path in enumerate(file_paths):\n",
    "        if i % 1000 == 0:\n",
    "            logger.debug(\"Loading face #%05d / %05d\", i + 1, n_faces)\n",
    "\n",
    "        # Checks if jpeg reading worked. Refer to issue #3594 for more\n",
    "        # details.\n",
    "        pil_img = Image.open(file_path)\n",
    "        pil_img = pil_img.crop(\n",
    "            (w_slice.start, h_slice.start, w_slice.stop, h_slice.stop)\n",
    "        )\n",
    "        if resize is not None:\n",
    "            pil_img = pil_img.resize((w, h))\n",
    "        face = np.asarray(pil_img, dtype=np.float32)\n",
    "\n",
    "        if face.ndim == 0:\n",
    "            raise RuntimeError(\n",
    "                \"Failed to read the image file %s, \"\n",
    "                \"Please make sure that libjpeg is installed\" % file_path\n",
    "            )\n",
    "\n",
    "        face /= 255.0  # scale uint8 coded colors to the [0.0, 1.0] floats\n",
    "        if not color:\n",
    "            # average the color channels to compute a gray levels\n",
    "            # representation\n",
    "            face = face.mean(axis=2)\n",
    "\n",
    "        faces[i, ...] = face\n",
    "\n",
    "    return faces\n",
    "\n",
    "\n",
    "#\n",
    "# Task #1:  Face Identification on picture with names\n",
    "#\n",
    "\n",
    "\n",
    "def _fetch_lfw_people(\n",
    "    data_folder_path, slice_=None, color=False, resize=None, min_faces_per_person=0\n",
    "):\n",
    "    \"\"\"Perform the actual data loading for the lfw people dataset\n",
    "    This operation is meant to be cached by a joblib wrapper.\n",
    "    \"\"\"\n",
    "    # scan the data folder content to retain people with more that\n",
    "    # `min_faces_per_person` face pictures\n",
    "    person_names, file_paths = [], []\n",
    "    for person_name in sorted(listdir(data_folder_path)):\n",
    "        folder_path = join(data_folder_path, person_name)\n",
    "        if not isdir(folder_path):\n",
    "            continue\n",
    "        paths = [join(folder_path, f) for f in sorted(listdir(folder_path))]\n",
    "        n_pictures = len(paths)\n",
    "        if n_pictures >= min_faces_per_person:\n",
    "            person_name = person_name.replace(\"_\", \" \")\n",
    "            person_names.extend([person_name] * n_pictures)\n",
    "            file_paths.extend(paths)\n",
    "\n",
    "    n_faces = len(file_paths)\n",
    "    if n_faces == 0:\n",
    "        raise ValueError(\n",
    "            \"min_faces_per_person=%d is too restrictive\" % min_faces_per_person\n",
    "        )\n",
    "\n",
    "    target_names = np.unique(person_names)\n",
    "    target = np.searchsorted(target_names, person_names)\n",
    "\n",
    "    faces = _load_imgs(file_paths, slice_, color, resize)\n",
    "\n",
    "    # shuffle the faces with a deterministic RNG scheme to avoid having\n",
    "    # all faces of the same person in a row, as it would break some\n",
    "    # cross validation and learning algorithms such as SGD and online\n",
    "    # k-means that make an IID assumption\n",
    "\n",
    "    indices = np.arange(n_faces)\n",
    "    np.random.RandomState(42).shuffle(indices)\n",
    "    faces, target = faces[indices], target[indices]\n",
    "    return faces, target, target_names\n",
    "\n",
    "\n",
    "def fetch_lfw_people(\n",
    "    *,\n",
    "    data_home=None,\n",
    "    funneled=True,\n",
    "    resize=0.5,\n",
    "    min_faces_per_person=0,\n",
    "    color=False,\n",
    "    slice_=(slice(70, 195), slice(78, 172)),\n",
    "    download_if_missing=True,\n",
    "    return_X_y=False,\n",
    "):\n",
    "    \"\"\"Load the Labeled Faces in the Wild (LFW) people dataset \\\n",
    "(classification).\n",
    "    Download it if necessary.\n",
    "    =================   =======================\n",
    "    Classes                                5749\n",
    "    Samples total                         13233\n",
    "    Dimensionality                         5828\n",
    "    Features            real, between 0 and 255\n",
    "    =================   =======================\n",
    "    Read more in the :ref:`User Guide <labeled_faces_in_the_wild_dataset>`.\n",
    "    Parameters\n",
    "    ----------\n",
    "    data_home : str, default=None\n",
    "        Specify another download and cache folder for the datasets. By default\n",
    "        all scikit-learn data is stored in '~/scikit_learn_data' subfolders.\n",
    "    funneled : bool, default=True\n",
    "        Download and use the funneled variant of the dataset.\n",
    "    resize : float or None, default=0.5\n",
    "        Ratio used to resize the each face picture. If `None`, no resizing is\n",
    "        performed.\n",
    "    min_faces_per_person : int, default=None\n",
    "        The extracted dataset will only retain pictures of people that have at\n",
    "        least `min_faces_per_person` different pictures.\n",
    "    color : bool, default=False\n",
    "        Keep the 3 RGB channels instead of averaging them to a single\n",
    "        gray level channel. If color is True the shape of the data has\n",
    "        one more dimension than the shape with color = False.\n",
    "    slice_ : tuple of slice, default=(slice(70, 195), slice(78, 172))\n",
    "        Provide a custom 2D slice (height, width) to extract the\n",
    "        'interesting' part of the jpeg files and avoid use statistical\n",
    "        correlation from the background.\n",
    "    download_if_missing : bool, default=True\n",
    "        If False, raise a IOError if the data is not locally available\n",
    "        instead of trying to download the data from the source site.\n",
    "    return_X_y : bool, default=False\n",
    "        If True, returns ``(dataset.data, dataset.target)`` instead of a Bunch\n",
    "        object. See below for more information about the `dataset.data` and\n",
    "        `dataset.target` object.\n",
    "        .. versionadded:: 0.20\n",
    "    Returns\n",
    "    -------\n",
    "    dataset : :class:`~sklearn.utils.Bunch`\n",
    "        Dictionary-like object, with the following attributes.\n",
    "        data : numpy array of shape (13233, 2914)\n",
    "            Each row corresponds to a ravelled face image\n",
    "            of original size 62 x 47 pixels.\n",
    "            Changing the ``slice_`` or resize parameters will change the\n",
    "            shape of the output.\n",
    "        images : numpy array of shape (13233, 62, 47)\n",
    "            Each row is a face image corresponding to one of the 5749 people in\n",
    "            the dataset. Changing the ``slice_``\n",
    "            or resize parameters will change the shape of the output.\n",
    "        target : numpy array of shape (13233,)\n",
    "            Labels associated to each face image.\n",
    "            Those labels range from 0-5748 and correspond to the person IDs.\n",
    "        target_names : numpy array of shape (5749,)\n",
    "            Names of all persons in the dataset.\n",
    "            Position in array corresponds to the person ID in the target array.\n",
    "        DESCR : str\n",
    "            Description of the Labeled Faces in the Wild (LFW) dataset.\n",
    "    (data, target) : tuple if ``return_X_y`` is True\n",
    "        A tuple of two ndarray. The first containing a 2D array of\n",
    "        shape (n_samples, n_features) with each row representing one\n",
    "        sample and each column representing the features. The second\n",
    "        ndarray of shape (n_samples,) containing the target samples.\n",
    "        .. versionadded:: 0.20\n",
    "    \"\"\"\n",
    "    lfw_home, data_folder_path = _check_fetch_lfw(\n",
    "        data_home=data_home, funneled=funneled, download_if_missing=download_if_missing\n",
    "    )\n",
    "    logger.debug(\"Loading LFW people faces from %s\", lfw_home)\n",
    "\n",
    "    # wrap the loader in a memoizing function that will return memmaped data\n",
    "    # arrays for optimal memory usage\n",
    "    m = Memory(location=lfw_home, compress=6, verbose=0)\n",
    "    load_func = m.cache(_fetch_lfw_people)\n",
    "\n",
    "    # load and memoize the pairs as np arrays\n",
    "    faces, target, target_names = load_func(\n",
    "        data_folder_path,\n",
    "        resize=resize,\n",
    "        min_faces_per_person=min_faces_per_person,\n",
    "        color=color,\n",
    "        slice_=slice_,\n",
    "    )\n",
    "\n",
    "    X = faces.reshape(len(faces), -1)\n",
    "\n",
    "    fdescr = load_descr(\"lfw.rst\")\n",
    "\n",
    "    if return_X_y:\n",
    "        return X, target\n",
    "\n",
    "    # pack the results as a Bunch instance\n",
    "    return Bunch(\n",
    "        data=X, images=faces, target=target, target_names=target_names, DESCR=fdescr\n",
    "    )\n",
    "\n",
    "\n",
    "#\n",
    "# Task #2:  Face Verification on pairs of face pictures\n",
    "#\n",
    "\n",
    "\n",
    "def _fetch_lfw_pairs(\n",
    "    index_file_path, data_folder_path, slice_=None, color=False, resize=None\n",
    "):\n",
    "    \"\"\"Perform the actual data loading for the LFW pairs dataset\n",
    "    This operation is meant to be cached by a joblib wrapper.\n",
    "    \"\"\"\n",
    "    # parse the index file to find the number of pairs to be able to allocate\n",
    "    # the right amount of memory before starting to decode the jpeg files\n",
    "    with open(index_file_path, \"rb\") as index_file:\n",
    "        split_lines = [ln.decode().strip().split(\"\\t\") for ln in index_file]\n",
    "    pair_specs = [sl for sl in split_lines if len(sl) > 2]\n",
    "    n_pairs = len(pair_specs)\n",
    "\n",
    "    # iterating over the metadata lines for each pair to find the filename to\n",
    "    # decode and load in memory\n",
    "    target = np.zeros(n_pairs, dtype=int)\n",
    "    file_paths = list()\n",
    "    for i, components in enumerate(pair_specs):\n",
    "        if len(components) == 3:\n",
    "            target[i] = 1\n",
    "            pair = (\n",
    "                (components[0], int(components[1]) - 1),\n",
    "                (components[0], int(components[2]) - 1),\n",
    "            )\n",
    "        elif len(components) == 4:\n",
    "            target[i] = 0\n",
    "            pair = (\n",
    "                (components[0], int(components[1]) - 1),\n",
    "                (components[2], int(components[3]) - 1),\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\"invalid line %d: %r\" % (i + 1, components))\n",
    "        for j, (name, idx) in enumerate(pair):\n",
    "            try:\n",
    "                person_folder = join(data_folder_path, name)\n",
    "            except TypeError:\n",
    "                person_folder = join(data_folder_path, str(name, \"UTF-8\"))\n",
    "            filenames = list(sorted(listdir(person_folder)))\n",
    "            file_path = join(person_folder, filenames[idx])\n",
    "            file_paths.append(file_path)\n",
    "\n",
    "    pairs = _load_imgs(file_paths, slice_, color, resize)\n",
    "    shape = list(pairs.shape)\n",
    "    n_faces = shape.pop(0)\n",
    "    shape.insert(0, 2)\n",
    "    shape.insert(0, n_faces // 2)\n",
    "    pairs.shape = shape\n",
    "\n",
    "    return pairs, target, np.array([\"Different persons\", \"Same person\"])\n",
    "\n",
    "\n",
    "def fetch_lfw_pairs(\n",
    "    *,\n",
    "    subset=\"train\",\n",
    "    data_home=None,\n",
    "    funneled=True,\n",
    "    resize=0.5,\n",
    "    color=False,\n",
    "    slice_=(slice(70, 195), slice(78, 172)),\n",
    "    download_if_missing=True,\n",
    "):\n",
    "    \"\"\"Load the Labeled Faces in the Wild (LFW) pairs dataset (classification).\n",
    "    Download it if necessary.\n",
    "    =================   =======================\n",
    "    Classes                                   2\n",
    "    Samples total                         13233\n",
    "    Dimensionality                         5828\n",
    "    Features            real, between 0 and 255\n",
    "    =================   =======================\n",
    "    In the official `README.txt`_ this task is described as the\n",
    "    \"Restricted\" task.  As I am not sure as to implement the\n",
    "    \"Unrestricted\" variant correctly, I left it as unsupported for now.\n",
    "      .. _`README.txt`: http://vis-www.cs.umass.edu/lfw/README.txt\n",
    "    The original images are 250 x 250 pixels, but the default slice and resize\n",
    "    arguments reduce them to 62 x 47.\n",
    "    Read more in the :ref:`User Guide <labeled_faces_in_the_wild_dataset>`.\n",
    "    Parameters\n",
    "    ----------\n",
    "    subset : {'train', 'test', '10_folds'}, default='train'\n",
    "        Select the dataset to load: 'train' for the development training\n",
    "        set, 'test' for the development test set, and '10_folds' for the\n",
    "        official evaluation set that is meant to be used with a 10-folds\n",
    "        cross validation.\n",
    "    data_home : str, default=None\n",
    "        Specify another download and cache folder for the datasets. By\n",
    "        default all scikit-learn data is stored in '~/scikit_learn_data'\n",
    "        subfolders.\n",
    "    funneled : bool, default=True\n",
    "        Download and use the funneled variant of the dataset.\n",
    "    resize : float, default=0.5\n",
    "        Ratio used to resize the each face picture.\n",
    "    color : bool, default=False\n",
    "        Keep the 3 RGB channels instead of averaging them to a single\n",
    "        gray level channel. If color is True the shape of the data has\n",
    "        one more dimension than the shape with color = False.\n",
    "    slice_ : tuple of slice, default=(slice(70, 195), slice(78, 172))\n",
    "        Provide a custom 2D slice (height, width) to extract the\n",
    "        'interesting' part of the jpeg files and avoid use statistical\n",
    "        correlation from the background.\n",
    "    download_if_missing : bool, default=True\n",
    "        If False, raise a IOError if the data is not locally available\n",
    "        instead of trying to download the data from the source site.\n",
    "    Returns\n",
    "    -------\n",
    "    data : :class:`~sklearn.utils.Bunch`\n",
    "        Dictionary-like object, with the following attributes.\n",
    "        data : ndarray of shape (2200, 5828). Shape depends on ``subset``.\n",
    "            Each row corresponds to 2 ravel'd face images\n",
    "            of original size 62 x 47 pixels.\n",
    "            Changing the ``slice_``, ``resize`` or ``subset`` parameters\n",
    "            will change the shape of the output.\n",
    "        pairs : ndarray of shape (2200, 2, 62, 47). Shape depends on ``subset``\n",
    "            Each row has 2 face images corresponding\n",
    "            to same or different person from the dataset\n",
    "            containing 5749 people. Changing the ``slice_``,\n",
    "            ``resize`` or ``subset`` parameters will change the shape of the\n",
    "            output.\n",
    "        target : numpy array of shape (2200,). Shape depends on ``subset``.\n",
    "            Labels associated to each pair of images.\n",
    "            The two label values being different persons or the same person.\n",
    "        target_names : numpy array of shape (2,)\n",
    "            Explains the target values of the target array.\n",
    "            0 corresponds to \"Different person\", 1 corresponds to \"same person\".\n",
    "        DESCR : str\n",
    "            Description of the Labeled Faces in the Wild (LFW) dataset.\n",
    "    \"\"\"\n",
    "    lfw_home, data_folder_path = _check_fetch_lfw(\n",
    "        data_home=data_home, funneled=funneled, download_if_missing=download_if_missing\n",
    "    )\n",
    "    logger.debug(\"Loading %s LFW pairs from %s\", subset, lfw_home)\n",
    "\n",
    "    # wrap the loader in a memoizing function that will return memmaped data\n",
    "    # arrays for optimal memory usage\n",
    "    m = Memory(location=lfw_home, compress=6, verbose=0)\n",
    "    load_func = m.cache(_fetch_lfw_pairs)\n",
    "\n",
    "    # select the right metadata file according to the requested subset\n",
    "    label_filenames = {\n",
    "        \"train\": \"pairsDevTrain.txt\",\n",
    "        \"test\": \"pairsDevTest.txt\",\n",
    "        \"10_folds\": \"pairs.txt\",\n",
    "    }\n",
    "    if subset not in label_filenames:\n",
    "        raise ValueError(\n",
    "            \"subset='%s' is invalid: should be one of %r\"\n",
    "            % (subset, list(sorted(label_filenames.keys())))\n",
    "        )\n",
    "    index_file_path = join(lfw_home, label_filenames[subset])\n",
    "\n",
    "    # load and memoize the pairs as np arrays\n",
    "    pairs, target, target_names = load_func(\n",
    "        index_file_path, data_folder_path, resize=resize, color=color, slice_=slice_\n",
    "    )\n",
    "\n",
    "    fdescr = load_descr(\"lfw.rst\")\n",
    "\n",
    "    # pack the results as a Bunch instance\n",
    "    return Bunch(\n",
    "        data=pairs.reshape(len(pairs), -1),\n",
    "        pairs=pairs,\n",
    "        target=target,\n",
    "        target_names=target_names,\n",
    "        DESCR=fdescr,\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
